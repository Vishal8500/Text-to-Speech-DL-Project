{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1IVij0FPNQJ5sgzxOuRSAT6rgH-B3pgYR",
      "authorship_tag": "ABX9TyN+1GdZiroSYYx6O2a687sL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vishal8500/Text-to-Speech-DL-Project/blob/main/Text_to_Speech.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xIKey66Mk2BV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a07ebec5-ac7d-4d4d-850d-252e290d605e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: librosa in /usr/local/lib/python3.11/dist-packages (0.11.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.11/dist-packages (from librosa) (3.0.1)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.60.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.14.1)\n",
            "Requirement already satisfied: scikit-learn>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.6.1)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.4.2)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (4.4.2)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.13.1)\n",
            "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.8.2)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.5.0.post1)\n",
            "Requirement already satisfied: typing_extensions>=4.1.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (4.12.2)\n",
            "Requirement already satisfied: lazy_loader>=0.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.1)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.51.0->librosa) (0.43.0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.1->librosa) (4.3.6)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.1->librosa) (2.32.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.1.0->librosa) (3.6.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.11/dist-packages (from soundfile>=0.12.1->librosa) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.22)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2025.1.31)\n"
          ]
        }
      ],
      "source": [
        "!pip install librosa numpy pandas matplotlib tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DATASET_PATH = \"/content/drive/MyDrive/LJSpeech-1.1\""
      ],
      "metadata": {
        "id": "l33XPnBflLD3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import librosa\n",
        "import librosa.display\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import json\n",
        "\n",
        "# Set dataset paths\n",
        "DATASET_PATH = \"/content/drive/MyDrive/LJSpeech-1.1\"  # Update this if needed\n",
        "AUDIO_PATH = os.path.join(DATASET_PATH, \"wavs\")\n",
        "METADATA_FILE = os.path.join(DATASET_PATH, \"metadata.csv\")\n",
        "OUTPUT_PATH = \"/content/processed_data\"\n",
        "\n",
        "# Create output directory\n",
        "os.makedirs(OUTPUT_PATH, exist_ok=True)\n",
        "\n",
        "# Load metadata\n",
        "df = pd.read_csv(METADATA_FILE, sep=\"|\", header=None, names=[\"file_id\", \"transcript\", \"normalized_text\"])\n",
        "\n",
        "# Handle missing or non-string values\n",
        "df[\"normalized_text\"] = df[\"normalized_text\"].fillna(\"\").astype(str)\n",
        "\n",
        "# Function to clean text\n",
        "def clean_text(text):\n",
        "    text = text.lower().strip()\n",
        "    text = text.replace(\",\", \"\").replace(\".\", \"\").replace(\";\", \"\").replace(\"?\", \"\")\n",
        "    return text\n",
        "\n",
        "# Apply text cleaning\n",
        "df[\"clean_text\"] = df[\"normalized_text\"].apply(clean_text)\n",
        "\n",
        "# Function to convert audio to Mel-spectrogram\n",
        "def process_audio(file_id, sr=22050, n_mels=80):\n",
        "    file_path = os.path.join(AUDIO_PATH, f\"{file_id}.wav\")\n",
        "    y, _ = librosa.load(file_path, sr=sr)\n",
        "\n",
        "    # Convert to Mel-spectrogram\n",
        "    mel_spec = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=n_mels)\n",
        "    mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
        "\n",
        "    return mel_spec_db\n",
        "\n",
        "# Process all audio files and save\n",
        "mel_data = {}\n",
        "\n",
        "for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
        "    file_id = row[\"file_id\"]\n",
        "    mel_spec_db = process_audio(file_id)\n",
        "\n",
        "    # Save spectrogram as numpy file\n",
        "    np.save(os.path.join(OUTPUT_PATH, f\"{file_id}.npy\"), mel_spec_db)\n",
        "\n",
        "    # Store metadata (text and spectrogram shape)\n",
        "    mel_data[file_id] = {\"text\": row[\"clean_text\"], \"mel_shape\": mel_spec_db.shape}\n",
        "\n",
        "# Save metadata to JSON\n",
        "with open(os.path.join(OUTPUT_PATH, \"metadata.json\"), \"w\") as f:\n",
        "    json.dump(mel_data, f, indent=4)\n",
        "\n",
        "print(\"✅ Preprocessing complete! Mel-spectrograms and metadata saved.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nozL5PmQliuY",
        "outputId": "b6b7f708-572d-4e48-d22f-c1be4f2c79f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 13100/13100 [2:53:43<00:00,  1.26it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Preprocessing complete! Mel-spectrograms and metadata saved.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "# Zip the processed_data folder\n",
        "shutil.make_archive('/content/processed_data', 'zip', '/content/processed_data')\n",
        "\n",
        "# Download the zip file\n",
        "from google.colab import files\n",
        "files.download('/content/processed_data.zip')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "ap0QgxHhIA28",
        "outputId": "5748eb83-cc8c-4d11-d897-f516fbcd7d98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/processed_data'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-ea5d8946a6db>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Zip the processed_data folder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_archive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/processed_data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'zip'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'/content/processed_data'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Download the zip file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/shutil.py\u001b[0m in \u001b[0;36mmake_archive\u001b[0;34m(base_name, format, root_dir, base_dir, verbose, dry_run, owner, group, logger)\u001b[0m\n\u001b[1;32m   1146\u001b[0m     \u001b[0msave_cwd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1147\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mroot_dir\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1148\u001b[0;31m         \u001b[0mstmd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mst_mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1149\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mstat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mS_ISDIR\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstmd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1150\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mNotADirectoryError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merrno\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mENOTDIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Not a directory'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroot_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/processed_data'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required libraries\n",
        "!pip install tensorflow librosa matplotlib\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iR2TvGdvP1fF",
        "outputId": "14e42238-267e-4c6d-e8ae-42bfb58e962f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.11/dist-packages (0.11.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (5.29.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.71.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.0.2)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.13.0)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.11/dist-packages (from librosa) (3.0.1)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.60.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.14.1)\n",
            "Requirement already satisfied: scikit-learn>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.6.1)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.4.2)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (4.4.2)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.13.1)\n",
            "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.8.2)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.5.0.post1)\n",
            "Requirement already satisfied: lazy_loader>=0.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.1.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.14.1)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.51.0->librosa) (0.43.0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.1->librosa) (4.3.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.1.31)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.1.0->librosa) (3.6.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.11/dist-packages (from soundfile>=0.12.1->librosa) (1.17.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: TensorFlow Dataset loading (you can use PyTorch similarly)\n",
        "import tensorflow as tf\n",
        "\n",
        "def load_data(metadata_path, audio_path, batch_size=32):\n",
        "    with open(metadata_path, \"r\") as f:\n",
        "        metadata = json.load(f)\n",
        "\n",
        "    text_data = [entry[\"text\"] for entry in metadata.values()]\n",
        "    mel_data = [np.load(os.path.join(audio_path, f\"{file_id}.npy\")) for file_id in metadata.keys()]\n",
        "\n",
        "    # Convert to TensorFlow Dataset (you can also use PyTorch Dataset)\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((text_data, mel_data))\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    return dataset\n"
      ],
      "metadata": {
        "id": "GQNNfQrHP2N_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install torch torchvision numpy tensorboard scipy matplotlib librosa"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SWQfEhLNcWLr",
        "outputId": "e76e9794-1d3a-4d81-fc27-9cf1fd23c6f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (1.14.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.11/dist-packages (0.11.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.1.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (1.71.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (3.7)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorboard) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (5.29.4)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (75.1.0)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (1.17.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (3.1.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.11/dist-packages (from librosa) (3.0.1)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.60.0)\n",
            "Requirement already satisfied: scikit-learn>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.6.1)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.4.2)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (4.4.2)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.13.1)\n",
            "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.8.2)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.5.0.post1)\n",
            "Requirement already satisfied: lazy_loader>=0.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.1.0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.51.0->librosa) (0.43.0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.1->librosa) (4.3.7)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.1->librosa) (2.32.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.1.0->librosa) (3.6.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.11/dist-packages (from soundfile>=0.12.1->librosa) (1.17.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard) (3.0.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.22)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2025.1.31)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m29.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m43.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python --version\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KTfKtzhEeNku",
        "outputId": "24f63833-603d-48f8-bfa9-523e3cad84d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.11.11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt-get update -y\n",
        "!sudo apt-get install python3.7 python3.7-distutils -y\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZxHDv817eP-w",
        "outputId": "36f836cf-b548-4baa-ed0a-3ee50009f0ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rGet:1 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "\r0% [Connecting to archive.ubuntu.com (91.189.91.83)] [1 InRelease 14.2 kB/129 k\r0% [Connecting to archive.ubuntu.com (91.189.91.83)] [Waiting for headers] [Con\r                                                                               \rGet:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "Get:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:5 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:6 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:7 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:9 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2,698 kB]\n",
            "Get:10 http://security.ubuntu.com/ubuntu jammy-security/multiverse amd64 Packages [47.7 kB]\n",
            "Get:11 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,238 kB]\n",
            "Get:12 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [3,813 kB]\n",
            "Get:13 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Get:14 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [1,380 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,536 kB]\n",
            "Get:17 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [8,780 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,037 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [4,028 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu jammy-updates/multiverse amd64 Packages [55.7 kB]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu jammy-backports/main amd64 Packages [82.7 kB]\n",
            "Get:22 http://archive.ubuntu.com/ubuntu jammy-backports/universe amd64 Packages [35.2 kB]\n",
            "Get:23 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,684 kB]\n",
            "Fetched 29.8 MB in 4s (7,905 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  libpython3.7-minimal libpython3.7-stdlib python3.7-lib2to3 python3.7-minimal\n",
            "Suggested packages:\n",
            "  python3.7-venv binfmt-support\n",
            "The following NEW packages will be installed:\n",
            "  libpython3.7-minimal libpython3.7-stdlib python3.7 python3.7-distutils\n",
            "  python3.7-lib2to3 python3.7-minimal\n",
            "0 upgraded, 6 newly installed, 0 to remove and 35 not upgraded.\n",
            "Need to get 4,983 kB of archives.\n",
            "After this operation, 18.9 MB of additional disk space will be used.\n",
            "Get:1 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 libpython3.7-minimal amd64 3.7.17-1+jammy1 [608 kB]\n",
            "Get:2 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 python3.7-minimal amd64 3.7.17-1+jammy1 [1,837 kB]\n",
            "Get:3 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 libpython3.7-stdlib amd64 3.7.17-1+jammy1 [1,864 kB]\n",
            "Get:4 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 python3.7 amd64 3.7.17-1+jammy1 [362 kB]\n",
            "Get:5 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 python3.7-lib2to3 all 3.7.17-1+jammy1 [124 kB]\n",
            "Get:6 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 python3.7-distutils all 3.7.17-1+jammy1 [189 kB]\n",
            "Fetched 4,983 kB in 2s (2,024 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 6.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package libpython3.7-minimal:amd64.\n",
            "(Reading database ... 126209 files and directories currently installed.)\n",
            "Preparing to unpack .../0-libpython3.7-minimal_3.7.17-1+jammy1_amd64.deb ...\n",
            "Unpacking libpython3.7-minimal:amd64 (3.7.17-1+jammy1) ...\n",
            "Selecting previously unselected package python3.7-minimal.\n",
            "Preparing to unpack .../1-python3.7-minimal_3.7.17-1+jammy1_amd64.deb ...\n",
            "Unpacking python3.7-minimal (3.7.17-1+jammy1) ...\n",
            "Selecting previously unselected package libpython3.7-stdlib:amd64.\n",
            "Preparing to unpack .../2-libpython3.7-stdlib_3.7.17-1+jammy1_amd64.deb ...\n",
            "Unpacking libpython3.7-stdlib:amd64 (3.7.17-1+jammy1) ...\n",
            "Selecting previously unselected package python3.7.\n",
            "Preparing to unpack .../3-python3.7_3.7.17-1+jammy1_amd64.deb ...\n",
            "Unpacking python3.7 (3.7.17-1+jammy1) ...\n",
            "Selecting previously unselected package python3.7-lib2to3.\n",
            "Preparing to unpack .../4-python3.7-lib2to3_3.7.17-1+jammy1_all.deb ...\n",
            "Unpacking python3.7-lib2to3 (3.7.17-1+jammy1) ...\n",
            "Selecting previously unselected package python3.7-distutils.\n",
            "Preparing to unpack .../5-python3.7-distutils_3.7.17-1+jammy1_all.deb ...\n",
            "Unpacking python3.7-distutils (3.7.17-1+jammy1) ...\n",
            "Setting up libpython3.7-minimal:amd64 (3.7.17-1+jammy1) ...\n",
            "Setting up python3.7-minimal (3.7.17-1+jammy1) ...\n",
            "Setting up python3.7-lib2to3 (3.7.17-1+jammy1) ...\n",
            "Setting up libpython3.7-stdlib:amd64 (3.7.17-1+jammy1) ...\n",
            "Setting up python3.7-distutils (3.7.17-1+jammy1) ...\n",
            "Setting up python3.7 (3.7.17-1+jammy1) ...\n",
            "Processing triggers for mailcap (3.70+nmu1ubuntu1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.7 1\n",
        "!sudo update-alternatives --config python3\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J-zjJkZefabC",
        "outputId": "eb47e73b-81ab-4ddc-edde-64bf9c7c7538"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 3 choices for the alternative python3 (providing /usr/bin/python3).\n",
            "\n",
            "  Selection    Path                 Priority   Status\n",
            "------------------------------------------------------------\n",
            "* 0            /usr/bin/python3.11   2         auto mode\n",
            "  1            /usr/bin/python3.10   1         manual mode\n",
            "  2            /usr/bin/python3.11   2         manual mode\n",
            "  3            /usr/bin/python3.7    1         manual mode\n",
            "\n",
            "Press <enter> to keep the current choice[*], or type selection number: 3\n",
            "update-alternatives: using /usr/bin/python3.7 to provide /usr/bin/python3 (python3) in manual mode\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python --version\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jfajXpjgerO8",
        "outputId": "7fa106a7-43f1-44a2-8893-76e572e3e7d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.7.17\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/NVIDIA/tacotron2.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4feXkeKvc9Oa",
        "outputId": "946cd34c-f3bf-4621-99be-b8b9bd1b098e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'tacotron2'...\n",
            "remote: Enumerating objects: 412, done.\u001b[K\n",
            "remote: Counting objects: 100% (4/4), done.\u001b[K\n",
            "remote: Compressing objects: 100% (3/3), done.\u001b[K\n",
            "remote: Total 412 (delta 2), reused 1 (delta 1), pack-reused 408 (from 2)\u001b[K\n",
            "Receiving objects: 100% (412/412), 2.70 MiB | 6.09 MiB/s, done.\n",
            "Resolving deltas: 100% (202/202), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd tacotron2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uRt9S11_d1BT",
        "outputId": "48795e48-9bcb-448c-c514-ccadcf33a2ab"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/tacotron2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cat /content/tacotron2/model.py\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MUSpOanYZslX",
        "outputId": "04206bd8-16a4-4dca-f2a5-a6ca4cb6829b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "from math import sqrt\n",
            "import torch\n",
            "from torch.autograd import Variable\n",
            "from torch import nn\n",
            "from torch.nn import functional as F\n",
            "from layers import ConvNorm, LinearNorm\n",
            "from utils import to_gpu, get_mask_from_lengths\n",
            "\n",
            "\n",
            "class LocationLayer(nn.Module):\n",
            "    def __init__(self, attention_n_filters, attention_kernel_size,\n",
            "                 attention_dim):\n",
            "        super(LocationLayer, self).__init__()\n",
            "        padding = int((attention_kernel_size - 1) / 2)\n",
            "        self.location_conv = ConvNorm(2, attention_n_filters,\n",
            "                                      kernel_size=attention_kernel_size,\n",
            "                                      padding=padding, bias=False, stride=1,\n",
            "                                      dilation=1)\n",
            "        self.location_dense = LinearNorm(attention_n_filters, attention_dim,\n",
            "                                         bias=False, w_init_gain='tanh')\n",
            "\n",
            "    def forward(self, attention_weights_cat):\n",
            "        processed_attention = self.location_conv(attention_weights_cat)\n",
            "        processed_attention = processed_attention.transpose(1, 2)\n",
            "        processed_attention = self.location_dense(processed_attention)\n",
            "        return processed_attention\n",
            "\n",
            "\n",
            "class Attention(nn.Module):\n",
            "    def __init__(self, attention_rnn_dim, embedding_dim, attention_dim,\n",
            "                 attention_location_n_filters, attention_location_kernel_size):\n",
            "        super(Attention, self).__init__()\n",
            "        self.query_layer = LinearNorm(attention_rnn_dim, attention_dim,\n",
            "                                      bias=False, w_init_gain='tanh')\n",
            "        self.memory_layer = LinearNorm(embedding_dim, attention_dim, bias=False,\n",
            "                                       w_init_gain='tanh')\n",
            "        self.v = LinearNorm(attention_dim, 1, bias=False)\n",
            "        self.location_layer = LocationLayer(attention_location_n_filters,\n",
            "                                            attention_location_kernel_size,\n",
            "                                            attention_dim)\n",
            "        self.score_mask_value = -float(\"inf\")\n",
            "\n",
            "    def get_alignment_energies(self, query, processed_memory,\n",
            "                               attention_weights_cat):\n",
            "        \"\"\"\n",
            "        PARAMS\n",
            "        ------\n",
            "        query: decoder output (batch, n_mel_channels * n_frames_per_step)\n",
            "        processed_memory: processed encoder outputs (B, T_in, attention_dim)\n",
            "        attention_weights_cat: cumulative and prev. att weights (B, 2, max_time)\n",
            "\n",
            "        RETURNS\n",
            "        -------\n",
            "        alignment (batch, max_time)\n",
            "        \"\"\"\n",
            "\n",
            "        processed_query = self.query_layer(query.unsqueeze(1))\n",
            "        processed_attention_weights = self.location_layer(attention_weights_cat)\n",
            "        energies = self.v(torch.tanh(\n",
            "            processed_query + processed_attention_weights + processed_memory))\n",
            "\n",
            "        energies = energies.squeeze(-1)\n",
            "        return energies\n",
            "\n",
            "    def forward(self, attention_hidden_state, memory, processed_memory,\n",
            "                attention_weights_cat, mask):\n",
            "        \"\"\"\n",
            "        PARAMS\n",
            "        ------\n",
            "        attention_hidden_state: attention rnn last output\n",
            "        memory: encoder outputs\n",
            "        processed_memory: processed encoder outputs\n",
            "        attention_weights_cat: previous and cummulative attention weights\n",
            "        mask: binary mask for padded data\n",
            "        \"\"\"\n",
            "        alignment = self.get_alignment_energies(\n",
            "            attention_hidden_state, processed_memory, attention_weights_cat)\n",
            "\n",
            "        if mask is not None:\n",
            "            alignment.data.masked_fill_(mask, self.score_mask_value)\n",
            "\n",
            "        attention_weights = F.softmax(alignment, dim=1)\n",
            "        attention_context = torch.bmm(attention_weights.unsqueeze(1), memory)\n",
            "        attention_context = attention_context.squeeze(1)\n",
            "\n",
            "        return attention_context, attention_weights\n",
            "\n",
            "\n",
            "class Prenet(nn.Module):\n",
            "    def __init__(self, in_dim, sizes):\n",
            "        super(Prenet, self).__init__()\n",
            "        in_sizes = [in_dim] + sizes[:-1]\n",
            "        self.layers = nn.ModuleList(\n",
            "            [LinearNorm(in_size, out_size, bias=False)\n",
            "             for (in_size, out_size) in zip(in_sizes, sizes)])\n",
            "\n",
            "    def forward(self, x):\n",
            "        for linear in self.layers:\n",
            "            x = F.dropout(F.relu(linear(x)), p=0.5, training=True)\n",
            "        return x\n",
            "\n",
            "\n",
            "class Postnet(nn.Module):\n",
            "    \"\"\"Postnet\n",
            "        - Five 1-d convolution with 512 channels and kernel size 5\n",
            "    \"\"\"\n",
            "\n",
            "    def __init__(self, hparams):\n",
            "        super(Postnet, self).__init__()\n",
            "        self.convolutions = nn.ModuleList()\n",
            "\n",
            "        self.convolutions.append(\n",
            "            nn.Sequential(\n",
            "                ConvNorm(hparams.n_mel_channels, hparams.postnet_embedding_dim,\n",
            "                         kernel_size=hparams.postnet_kernel_size, stride=1,\n",
            "                         padding=int((hparams.postnet_kernel_size - 1) / 2),\n",
            "                         dilation=1, w_init_gain='tanh'),\n",
            "                nn.BatchNorm1d(hparams.postnet_embedding_dim))\n",
            "        )\n",
            "\n",
            "        for i in range(1, hparams.postnet_n_convolutions - 1):\n",
            "            self.convolutions.append(\n",
            "                nn.Sequential(\n",
            "                    ConvNorm(hparams.postnet_embedding_dim,\n",
            "                             hparams.postnet_embedding_dim,\n",
            "                             kernel_size=hparams.postnet_kernel_size, stride=1,\n",
            "                             padding=int((hparams.postnet_kernel_size - 1) / 2),\n",
            "                             dilation=1, w_init_gain='tanh'),\n",
            "                    nn.BatchNorm1d(hparams.postnet_embedding_dim))\n",
            "            )\n",
            "\n",
            "        self.convolutions.append(\n",
            "            nn.Sequential(\n",
            "                ConvNorm(hparams.postnet_embedding_dim, hparams.n_mel_channels,\n",
            "                         kernel_size=hparams.postnet_kernel_size, stride=1,\n",
            "                         padding=int((hparams.postnet_kernel_size - 1) / 2),\n",
            "                         dilation=1, w_init_gain='linear'),\n",
            "                nn.BatchNorm1d(hparams.n_mel_channels))\n",
            "            )\n",
            "\n",
            "    def forward(self, x):\n",
            "        for i in range(len(self.convolutions) - 1):\n",
            "            x = F.dropout(torch.tanh(self.convolutions[i](x)), 0.5, self.training)\n",
            "        x = F.dropout(self.convolutions[-1](x), 0.5, self.training)\n",
            "\n",
            "        return x\n",
            "\n",
            "\n",
            "class Encoder(nn.Module):\n",
            "    \"\"\"Encoder module:\n",
            "        - Three 1-d convolution banks\n",
            "        - Bidirectional LSTM\n",
            "    \"\"\"\n",
            "    def __init__(self, hparams):\n",
            "        super(Encoder, self).__init__()\n",
            "\n",
            "        convolutions = []\n",
            "        for _ in range(hparams.encoder_n_convolutions):\n",
            "            conv_layer = nn.Sequential(\n",
            "                ConvNorm(hparams.encoder_embedding_dim,\n",
            "                         hparams.encoder_embedding_dim,\n",
            "                         kernel_size=hparams.encoder_kernel_size, stride=1,\n",
            "                         padding=int((hparams.encoder_kernel_size - 1) / 2),\n",
            "                         dilation=1, w_init_gain='relu'),\n",
            "                nn.BatchNorm1d(hparams.encoder_embedding_dim))\n",
            "            convolutions.append(conv_layer)\n",
            "        self.convolutions = nn.ModuleList(convolutions)\n",
            "\n",
            "        self.lstm = nn.LSTM(hparams.encoder_embedding_dim,\n",
            "                            int(hparams.encoder_embedding_dim / 2), 1,\n",
            "                            batch_first=True, bidirectional=True)\n",
            "\n",
            "    def forward(self, x, input_lengths):\n",
            "        for conv in self.convolutions:\n",
            "            x = F.dropout(F.relu(conv(x)), 0.5, self.training)\n",
            "\n",
            "        x = x.transpose(1, 2)\n",
            "\n",
            "        # pytorch tensor are not reversible, hence the conversion\n",
            "        input_lengths = input_lengths.cpu().numpy()\n",
            "        x = nn.utils.rnn.pack_padded_sequence(\n",
            "            x, input_lengths, batch_first=True)\n",
            "\n",
            "        self.lstm.flatten_parameters()\n",
            "        outputs, _ = self.lstm(x)\n",
            "\n",
            "        outputs, _ = nn.utils.rnn.pad_packed_sequence(\n",
            "            outputs, batch_first=True)\n",
            "\n",
            "        return outputs\n",
            "\n",
            "    def inference(self, x):\n",
            "        for conv in self.convolutions:\n",
            "            x = F.dropout(F.relu(conv(x)), 0.5, self.training)\n",
            "\n",
            "        x = x.transpose(1, 2)\n",
            "\n",
            "        self.lstm.flatten_parameters()\n",
            "        outputs, _ = self.lstm(x)\n",
            "\n",
            "        return outputs\n",
            "\n",
            "\n",
            "class Decoder(nn.Module):\n",
            "    def __init__(self, hparams):\n",
            "        super(Decoder, self).__init__()\n",
            "        self.n_mel_channels = hparams.n_mel_channels\n",
            "        self.n_frames_per_step = hparams.n_frames_per_step\n",
            "        self.encoder_embedding_dim = hparams.encoder_embedding_dim\n",
            "        self.attention_rnn_dim = hparams.attention_rnn_dim\n",
            "        self.decoder_rnn_dim = hparams.decoder_rnn_dim\n",
            "        self.prenet_dim = hparams.prenet_dim\n",
            "        self.max_decoder_steps = hparams.max_decoder_steps\n",
            "        self.gate_threshold = hparams.gate_threshold\n",
            "        self.p_attention_dropout = hparams.p_attention_dropout\n",
            "        self.p_decoder_dropout = hparams.p_decoder_dropout\n",
            "\n",
            "        self.prenet = Prenet(\n",
            "            hparams.n_mel_channels * hparams.n_frames_per_step,\n",
            "            [hparams.prenet_dim, hparams.prenet_dim])\n",
            "\n",
            "        self.attention_rnn = nn.LSTMCell(\n",
            "            hparams.prenet_dim + hparams.encoder_embedding_dim,\n",
            "            hparams.attention_rnn_dim)\n",
            "\n",
            "        self.attention_layer = Attention(\n",
            "            hparams.attention_rnn_dim, hparams.encoder_embedding_dim,\n",
            "            hparams.attention_dim, hparams.attention_location_n_filters,\n",
            "            hparams.attention_location_kernel_size)\n",
            "\n",
            "        self.decoder_rnn = nn.LSTMCell(\n",
            "            hparams.attention_rnn_dim + hparams.encoder_embedding_dim,\n",
            "            hparams.decoder_rnn_dim, 1)\n",
            "\n",
            "        self.linear_projection = LinearNorm(\n",
            "            hparams.decoder_rnn_dim + hparams.encoder_embedding_dim,\n",
            "            hparams.n_mel_channels * hparams.n_frames_per_step)\n",
            "\n",
            "        self.gate_layer = LinearNorm(\n",
            "            hparams.decoder_rnn_dim + hparams.encoder_embedding_dim, 1,\n",
            "            bias=True, w_init_gain='sigmoid')\n",
            "\n",
            "    def get_go_frame(self, memory):\n",
            "        \"\"\" Gets all zeros frames to use as first decoder input\n",
            "        PARAMS\n",
            "        ------\n",
            "        memory: decoder outputs\n",
            "\n",
            "        RETURNS\n",
            "        -------\n",
            "        decoder_input: all zeros frames\n",
            "        \"\"\"\n",
            "        B = memory.size(0)\n",
            "        decoder_input = Variable(memory.data.new(\n",
            "            B, self.n_mel_channels * self.n_frames_per_step).zero_())\n",
            "        return decoder_input\n",
            "\n",
            "    def initialize_decoder_states(self, memory, mask):\n",
            "        \"\"\" Initializes attention rnn states, decoder rnn states, attention\n",
            "        weights, attention cumulative weights, attention context, stores memory\n",
            "        and stores processed memory\n",
            "        PARAMS\n",
            "        ------\n",
            "        memory: Encoder outputs\n",
            "        mask: Mask for padded data if training, expects None for inference\n",
            "        \"\"\"\n",
            "        B = memory.size(0)\n",
            "        MAX_TIME = memory.size(1)\n",
            "\n",
            "        self.attention_hidden = Variable(memory.data.new(\n",
            "            B, self.attention_rnn_dim).zero_())\n",
            "        self.attention_cell = Variable(memory.data.new(\n",
            "            B, self.attention_rnn_dim).zero_())\n",
            "\n",
            "        self.decoder_hidden = Variable(memory.data.new(\n",
            "            B, self.decoder_rnn_dim).zero_())\n",
            "        self.decoder_cell = Variable(memory.data.new(\n",
            "            B, self.decoder_rnn_dim).zero_())\n",
            "\n",
            "        self.attention_weights = Variable(memory.data.new(\n",
            "            B, MAX_TIME).zero_())\n",
            "        self.attention_weights_cum = Variable(memory.data.new(\n",
            "            B, MAX_TIME).zero_())\n",
            "        self.attention_context = Variable(memory.data.new(\n",
            "            B, self.encoder_embedding_dim).zero_())\n",
            "\n",
            "        self.memory = memory\n",
            "        self.processed_memory = self.attention_layer.memory_layer(memory)\n",
            "        self.mask = mask\n",
            "\n",
            "    def parse_decoder_inputs(self, decoder_inputs):\n",
            "        \"\"\" Prepares decoder inputs, i.e. mel outputs\n",
            "        PARAMS\n",
            "        ------\n",
            "        decoder_inputs: inputs used for teacher-forced training, i.e. mel-specs\n",
            "\n",
            "        RETURNS\n",
            "        -------\n",
            "        inputs: processed decoder inputs\n",
            "\n",
            "        \"\"\"\n",
            "        # (B, n_mel_channels, T_out) -> (B, T_out, n_mel_channels)\n",
            "        decoder_inputs = decoder_inputs.transpose(1, 2)\n",
            "        decoder_inputs = decoder_inputs.view(\n",
            "            decoder_inputs.size(0),\n",
            "            int(decoder_inputs.size(1)/self.n_frames_per_step), -1)\n",
            "        # (B, T_out, n_mel_channels) -> (T_out, B, n_mel_channels)\n",
            "        decoder_inputs = decoder_inputs.transpose(0, 1)\n",
            "        return decoder_inputs\n",
            "\n",
            "    def parse_decoder_outputs(self, mel_outputs, gate_outputs, alignments):\n",
            "        \"\"\" Prepares decoder outputs for output\n",
            "        PARAMS\n",
            "        ------\n",
            "        mel_outputs:\n",
            "        gate_outputs: gate output energies\n",
            "        alignments:\n",
            "\n",
            "        RETURNS\n",
            "        -------\n",
            "        mel_outputs:\n",
            "        gate_outpust: gate output energies\n",
            "        alignments:\n",
            "        \"\"\"\n",
            "        # (T_out, B) -> (B, T_out)\n",
            "        alignments = torch.stack(alignments).transpose(0, 1)\n",
            "        # (T_out, B) -> (B, T_out)\n",
            "        gate_outputs = torch.stack(gate_outputs).transpose(0, 1)\n",
            "        gate_outputs = gate_outputs.contiguous()\n",
            "        # (T_out, B, n_mel_channels) -> (B, T_out, n_mel_channels)\n",
            "        mel_outputs = torch.stack(mel_outputs).transpose(0, 1).contiguous()\n",
            "        # decouple frames per step\n",
            "        mel_outputs = mel_outputs.view(\n",
            "            mel_outputs.size(0), -1, self.n_mel_channels)\n",
            "        # (B, T_out, n_mel_channels) -> (B, n_mel_channels, T_out)\n",
            "        mel_outputs = mel_outputs.transpose(1, 2)\n",
            "\n",
            "        return mel_outputs, gate_outputs, alignments\n",
            "\n",
            "    def decode(self, decoder_input):\n",
            "        \"\"\" Decoder step using stored states, attention and memory\n",
            "        PARAMS\n",
            "        ------\n",
            "        decoder_input: previous mel output\n",
            "\n",
            "        RETURNS\n",
            "        -------\n",
            "        mel_output:\n",
            "        gate_output: gate output energies\n",
            "        attention_weights:\n",
            "        \"\"\"\n",
            "        cell_input = torch.cat((decoder_input, self.attention_context), -1)\n",
            "        self.attention_hidden, self.attention_cell = self.attention_rnn(\n",
            "            cell_input, (self.attention_hidden, self.attention_cell))\n",
            "        self.attention_hidden = F.dropout(\n",
            "            self.attention_hidden, self.p_attention_dropout, self.training)\n",
            "\n",
            "        attention_weights_cat = torch.cat(\n",
            "            (self.attention_weights.unsqueeze(1),\n",
            "             self.attention_weights_cum.unsqueeze(1)), dim=1)\n",
            "        self.attention_context, self.attention_weights = self.attention_layer(\n",
            "            self.attention_hidden, self.memory, self.processed_memory,\n",
            "            attention_weights_cat, self.mask)\n",
            "\n",
            "        self.attention_weights_cum += self.attention_weights\n",
            "        decoder_input = torch.cat(\n",
            "            (self.attention_hidden, self.attention_context), -1)\n",
            "        self.decoder_hidden, self.decoder_cell = self.decoder_rnn(\n",
            "            decoder_input, (self.decoder_hidden, self.decoder_cell))\n",
            "        self.decoder_hidden = F.dropout(\n",
            "            self.decoder_hidden, self.p_decoder_dropout, self.training)\n",
            "\n",
            "        decoder_hidden_attention_context = torch.cat(\n",
            "            (self.decoder_hidden, self.attention_context), dim=1)\n",
            "        decoder_output = self.linear_projection(\n",
            "            decoder_hidden_attention_context)\n",
            "\n",
            "        gate_prediction = self.gate_layer(decoder_hidden_attention_context)\n",
            "        return decoder_output, gate_prediction, self.attention_weights\n",
            "\n",
            "    def forward(self, memory, decoder_inputs, memory_lengths):\n",
            "        \"\"\" Decoder forward pass for training\n",
            "        PARAMS\n",
            "        ------\n",
            "        memory: Encoder outputs\n",
            "        decoder_inputs: Decoder inputs for teacher forcing. i.e. mel-specs\n",
            "        memory_lengths: Encoder output lengths for attention masking.\n",
            "\n",
            "        RETURNS\n",
            "        -------\n",
            "        mel_outputs: mel outputs from the decoder\n",
            "        gate_outputs: gate outputs from the decoder\n",
            "        alignments: sequence of attention weights from the decoder\n",
            "        \"\"\"\n",
            "\n",
            "        decoder_input = self.get_go_frame(memory).unsqueeze(0)\n",
            "        decoder_inputs = self.parse_decoder_inputs(decoder_inputs)\n",
            "        decoder_inputs = torch.cat((decoder_input, decoder_inputs), dim=0)\n",
            "        decoder_inputs = self.prenet(decoder_inputs)\n",
            "\n",
            "        self.initialize_decoder_states(\n",
            "            memory, mask=~get_mask_from_lengths(memory_lengths))\n",
            "\n",
            "        mel_outputs, gate_outputs, alignments = [], [], []\n",
            "        while len(mel_outputs) < decoder_inputs.size(0) - 1:\n",
            "            decoder_input = decoder_inputs[len(mel_outputs)]\n",
            "            mel_output, gate_output, attention_weights = self.decode(\n",
            "                decoder_input)\n",
            "            mel_outputs += [mel_output.squeeze(1)]\n",
            "            gate_outputs += [gate_output.squeeze(1)]\n",
            "            alignments += [attention_weights]\n",
            "\n",
            "        mel_outputs, gate_outputs, alignments = self.parse_decoder_outputs(\n",
            "            mel_outputs, gate_outputs, alignments)\n",
            "\n",
            "        return mel_outputs, gate_outputs, alignments\n",
            "\n",
            "    def inference(self, memory):\n",
            "        \"\"\" Decoder inference\n",
            "        PARAMS\n",
            "        ------\n",
            "        memory: Encoder outputs\n",
            "\n",
            "        RETURNS\n",
            "        -------\n",
            "        mel_outputs: mel outputs from the decoder\n",
            "        gate_outputs: gate outputs from the decoder\n",
            "        alignments: sequence of attention weights from the decoder\n",
            "        \"\"\"\n",
            "        decoder_input = self.get_go_frame(memory)\n",
            "\n",
            "        self.initialize_decoder_states(memory, mask=None)\n",
            "\n",
            "        mel_outputs, gate_outputs, alignments = [], [], []\n",
            "        while True:\n",
            "            decoder_input = self.prenet(decoder_input)\n",
            "            mel_output, gate_output, alignment = self.decode(decoder_input)\n",
            "\n",
            "            mel_outputs += [mel_output.squeeze(1)]\n",
            "            gate_outputs += [gate_output]\n",
            "            alignments += [alignment]\n",
            "\n",
            "            if torch.sigmoid(gate_output.data) > self.gate_threshold:\n",
            "                break\n",
            "            elif len(mel_outputs) == self.max_decoder_steps:\n",
            "                print(\"Warning! Reached max decoder steps\")\n",
            "                break\n",
            "\n",
            "            decoder_input = mel_output\n",
            "\n",
            "        mel_outputs, gate_outputs, alignments = self.parse_decoder_outputs(\n",
            "            mel_outputs, gate_outputs, alignments)\n",
            "\n",
            "        return mel_outputs, gate_outputs, alignments\n",
            "\n",
            "\n",
            "class Tacotron2(nn.Module):\n",
            "    def __init__(self, hparams):\n",
            "        super(Tacotron2, self).__init__()\n",
            "        self.mask_padding = hparams.mask_padding\n",
            "        self.fp16_run = hparams.fp16_run\n",
            "        self.n_mel_channels = hparams.n_mel_channels\n",
            "        self.n_frames_per_step = hparams.n_frames_per_step\n",
            "        self.embedding = nn.Embedding(\n",
            "            hparams.n_symbols, hparams.symbols_embedding_dim)\n",
            "        std = sqrt(2.0 / (hparams.n_symbols + hparams.symbols_embedding_dim))\n",
            "        val = sqrt(3.0) * std  # uniform bounds for std\n",
            "        self.embedding.weight.data.uniform_(-val, val)\n",
            "        self.encoder = Encoder(hparams)\n",
            "        self.decoder = Decoder(hparams)\n",
            "        self.postnet = Postnet(hparams)\n",
            "\n",
            "    def parse_batch(self, batch):\n",
            "        text_padded, input_lengths, mel_padded, gate_padded, \\\n",
            "            output_lengths = batch\n",
            "        text_padded = to_gpu(text_padded).long()\n",
            "        input_lengths = to_gpu(input_lengths).long()\n",
            "        max_len = torch.max(input_lengths.data).item()\n",
            "        mel_padded = to_gpu(mel_padded).float()\n",
            "        gate_padded = to_gpu(gate_padded).float()\n",
            "        output_lengths = to_gpu(output_lengths).long()\n",
            "\n",
            "        return (\n",
            "            (text_padded, input_lengths, mel_padded, max_len, output_lengths),\n",
            "            (mel_padded, gate_padded))\n",
            "\n",
            "    def parse_output(self, outputs, output_lengths=None):\n",
            "        if self.mask_padding and output_lengths is not None:\n",
            "            mask = ~get_mask_from_lengths(output_lengths)\n",
            "            mask = mask.expand(self.n_mel_channels, mask.size(0), mask.size(1))\n",
            "            mask = mask.permute(1, 0, 2)\n",
            "\n",
            "            outputs[0].data.masked_fill_(mask, 0.0)\n",
            "            outputs[1].data.masked_fill_(mask, 0.0)\n",
            "            outputs[2].data.masked_fill_(mask[:, 0, :], 1e3)  # gate energies\n",
            "\n",
            "        return outputs\n",
            "\n",
            "    def forward(self, inputs):\n",
            "        text_inputs, text_lengths, mels, max_len, output_lengths = inputs\n",
            "        text_lengths, output_lengths = text_lengths.data, output_lengths.data\n",
            "\n",
            "        embedded_inputs = self.embedding(text_inputs).transpose(1, 2)\n",
            "\n",
            "        encoder_outputs = self.encoder(embedded_inputs, text_lengths)\n",
            "\n",
            "        mel_outputs, gate_outputs, alignments = self.decoder(\n",
            "            encoder_outputs, mels, memory_lengths=text_lengths)\n",
            "\n",
            "        mel_outputs_postnet = self.postnet(mel_outputs)\n",
            "        mel_outputs_postnet = mel_outputs + mel_outputs_postnet\n",
            "\n",
            "        return self.parse_output(\n",
            "            [mel_outputs, mel_outputs_postnet, gate_outputs, alignments],\n",
            "            output_lengths)\n",
            "\n",
            "    def inference(self, inputs):\n",
            "        embedded_inputs = self.embedding(inputs).transpose(1, 2)\n",
            "        encoder_outputs = self.encoder.inference(embedded_inputs)\n",
            "        mel_outputs, gate_outputs, alignments = self.decoder.inference(\n",
            "            encoder_outputs)\n",
            "\n",
            "        mel_outputs_postnet = self.postnet(mel_outputs)\n",
            "        mel_outputs_postnet = mel_outputs + mel_outputs_postnet\n",
            "\n",
            "        outputs = self.parse_output(\n",
            "            [mel_outputs, mel_outputs_postnet, gate_outputs, alignments])\n",
            "\n",
            "        return outputs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from model import Tacotron2\n",
        "from hparams import create_hparams\n",
        "\n",
        "hparams = create_hparams()\n",
        "model = Tacotron2(hparams)\n",
        "\n",
        "# Print model structure\n",
        "print(model)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "NGMpGNa8Zxfe",
        "outputId": "89ced52c-acee-4e82-bad6-2fbf635fab7e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "module 'tensorflow' has no attribute 'contrib'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-4995d2a78001>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mhparams\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcreate_hparams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mhparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_hparams\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTacotron2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/tacotron2/hparams.py\u001b[0m in \u001b[0;36mcreate_hparams\u001b[0;34m(hparams_string, verbose)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;34m\"\"\"Create model hyperparameters. Parse nondefault from given string.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     hparams = tf.contrib.training.HParams(\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0;31m################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;31m# Experiment Parameters        #\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'contrib'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://bootstrap.pypa.io/pip/3.7/get-pip.py\n",
        "!python3.7 get-pip.py\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gyo7tuptgWwU",
        "outputId": "7ceae45a-4f4d-4a77-d8bd-7fe3930d4c67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-03-25 15:53:56--  https://bootstrap.pypa.io/pip/3.7/get-pip.py\n",
            "Resolving bootstrap.pypa.io (bootstrap.pypa.io)... 151.101.0.175, 151.101.64.175, 151.101.128.175, ...\n",
            "Connecting to bootstrap.pypa.io (bootstrap.pypa.io)|151.101.0.175|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2636033 (2.5M) [text/x-python]\n",
            "Saving to: ‘get-pip.py’\n",
            "\n",
            "\rget-pip.py            0%[                    ]       0  --.-KB/s               \rget-pip.py          100%[===================>]   2.51M  --.-KB/s    in 0.03s   \n",
            "\n",
            "2025-03-25 15:53:56 (85.3 MB/s) - ‘get-pip.py’ saved [2636033/2636033]\n",
            "\n",
            "Collecting pip<24.1\n",
            "  Downloading pip-24.0-py3-none-any.whl.metadata (3.6 kB)\n",
            "Collecting setuptools\n",
            "  Downloading setuptools-68.0.0-py3-none-any.whl.metadata (6.4 kB)\n",
            "Collecting wheel\n",
            "  Downloading wheel-0.42.0-py3-none-any.whl.metadata (2.2 kB)\n",
            "Downloading pip-24.0-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading setuptools-68.0.0-py3-none-any.whl (804 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m804.0/804.0 kB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wheel-0.42.0-py3-none-any.whl (65 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.4/65.4 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: wheel, setuptools, pip\n",
            "Successfully installed pip-24.0 setuptools-68.0.0 wheel-0.42.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy==1.18.5\n",
        "!pip install matplotlib==3.2.2\n",
        "!pip install tensorflow==1.15.2\n",
        "!pip install inflect==0.2.5\n",
        "!pip install librosa==0.6.0\n",
        "!pip install scipy==1.0.0\n",
        "!pip install Unidecode==1.0.22\n",
        "!pip install pillow\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "2hcDDN0Lg34H",
        "outputId": "f01213f7-6dfd-4929-8509-5a3cd6d0a13b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting numpy==1.18.5\n",
            "  Downloading numpy-1.18.5.zip (5.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m31.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mPreparing metadata \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mpyproject.toml\u001b[0m\u001b[1;32m)\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n",
            "Collecting matplotlib==3.2.2\n",
            "  Downloading matplotlib-3.2.2.tar.gz (40.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.3/40.3 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.2.2) (0.12.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.2.2) (1.4.8)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.2.2) (2.0.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.2.2) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.2.2) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.1->matplotlib==3.2.2) (1.17.0)\n",
            "Building wheels for collected packages: matplotlib\n",
            "  Building wheel for matplotlib (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for matplotlib: filename=matplotlib-3.2.2-cp311-cp311-linux_x86_64.whl size=11963198 sha256=d4242bb4d8396ebf6cad725c331fb406aacf1ad40528f232960e9ff89e689635\n",
            "  Stored in directory: /root/.cache/pip/wheels/24/49/a7/b00a675ec9397a601599e7d1e21489f0d9d6eb7871f5f38957\n",
            "Successfully built matplotlib\n",
            "Installing collected packages: matplotlib\n",
            "  Attempting uninstall: matplotlib\n",
            "    Found existing installation: matplotlib 3.10.0\n",
            "    Uninstalling matplotlib-3.10.0:\n",
            "      Successfully uninstalled matplotlib-3.10.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "plotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.2.2 which is incompatible.\n",
            "arviz 0.21.0 requires matplotlib>=3.5, but you have matplotlib 3.2.2 which is incompatible.\n",
            "bigframes 1.41.0 requires matplotlib>=3.7.1, but you have matplotlib 3.2.2 which is incompatible.\n",
            "seaborn 0.13.2 requires matplotlib!=3.6.1,>=3.4, but you have matplotlib 3.2.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed matplotlib-3.2.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "matplotlib",
                  "mpl_toolkits"
                ]
              },
              "id": "b98dd884917f451caf47da88828fc3fa"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement tensorflow==1.15.2 (from versions: 2.12.0rc0, 2.12.0rc1, 2.12.0, 2.12.1, 2.13.0rc0, 2.13.0rc1, 2.13.0rc2, 2.13.0, 2.13.1, 2.14.0rc0, 2.14.0rc1, 2.14.0, 2.14.1, 2.15.0rc0, 2.15.0rc1, 2.15.0, 2.15.0.post1, 2.15.1, 2.16.0rc0, 2.16.1, 2.16.2, 2.17.0rc0, 2.17.0rc1, 2.17.0, 2.17.1, 2.18.0rc0, 2.18.0rc1, 2.18.0rc2, 2.18.0, 2.18.1, 2.19.0rc0, 2.19.0)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for tensorflow==1.15.2\u001b[0m\u001b[31m\n",
            "\u001b[0mCollecting inflect==0.2.5\n",
            "  Downloading inflect-0.2.5-py2.py3-none-any.whl.metadata (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.3/50.3 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading inflect-0.2.5-py2.py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.0/59.0 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: inflect\n",
            "  Attempting uninstall: inflect\n",
            "    Found existing installation: inflect 7.5.0\n",
            "    Uninstalling inflect-7.5.0:\n",
            "      Successfully uninstalled inflect-7.5.0\n",
            "Successfully installed inflect-0.2.5\n",
            "Collecting librosa==0.6.0\n",
            "  Downloading librosa-0.6.0.tar.gz (1.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: audioread>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from librosa==0.6.0) (3.0.1)\n",
            "Requirement already satisfied: numpy>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from librosa==0.6.0) (2.0.2)\n",
            "Requirement already satisfied: scipy>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from librosa==0.6.0) (1.14.1)\n",
            "Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from librosa==0.6.0) (1.6.1)\n",
            "Requirement already satisfied: joblib>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from librosa==0.6.0) (1.4.2)\n",
            "Requirement already satisfied: decorator>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from librosa==0.6.0) (4.4.2)\n",
            "Requirement already satisfied: six>=1.3 in /usr/local/lib/python3.11/dist-packages (from librosa==0.6.0) (1.17.0)\n",
            "Collecting resampy>=0.2.0 (from librosa==0.6.0)\n",
            "  Downloading resampy-0.4.3-py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: numba>=0.53 in /usr/local/lib/python3.11/dist-packages (from resampy>=0.2.0->librosa==0.6.0) (0.60.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn!=0.19.0,>=0.14.0->librosa==0.6.0) (3.6.0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.53->resampy>=0.2.0->librosa==0.6.0) (0.43.0)\n",
            "Downloading resampy-0.4.3-py3-none-any.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m76.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: librosa\n",
            "  Building wheel for librosa (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for librosa: filename=librosa-0.6.0-py3-none-any.whl size=1553465 sha256=cd18f0f352995dbb115c92811a70738d19c30463ac6bd01eeddcb8911c676c89\n",
            "  Stored in directory: /root/.cache/pip/wheels/c0/3b/16/a59905a55d8f8323c1c3869b103752d7bd2d6b876244054269\n",
            "Successfully built librosa\n",
            "Installing collected packages: resampy, librosa\n",
            "  Attempting uninstall: librosa\n",
            "    Found existing installation: librosa 0.11.0\n",
            "    Uninstalling librosa-0.11.0:\n",
            "      Successfully uninstalled librosa-0.11.0\n",
            "Successfully installed librosa-0.6.0 resampy-0.4.3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "librosa"
                ]
              },
              "id": "983a19a74c854ebfbb11c0796407dcac"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scipy==1.0.0\n",
            "  Downloading scipy-1.0.0.tar.gz (15.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.2/15.2 MB\u001b[0m \u001b[31m27.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: scipy\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py bdist_wheel\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Building wheel for scipy (setup.py) ... \u001b[?25lerror\n",
            "\u001b[31m  ERROR: Failed building wheel for scipy\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[?25h  Running setup.py clean for scipy\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py clean\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "\u001b[31m  ERROR: Failed cleaning build dir for scipy\u001b[0m\u001b[31m\n",
            "\u001b[0mFailed to build scipy\n",
            "\u001b[31mERROR: ERROR: Failed to build installable wheels for some pyproject.toml based projects (scipy)\u001b[0m\u001b[31m\n",
            "\u001b[0mCollecting Unidecode==1.0.22\n",
            "  Downloading Unidecode-1.0.22-py2.py3-none-any.whl.metadata (7.4 kB)\n",
            "Downloading Unidecode-1.0.22-py2.py3-none-any.whl (235 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.4/235.4 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: Unidecode\n",
            "Successfully installed Unidecode-1.0.22\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (11.1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install -y libsndfile1 ffmpeg\n",
        "!pip install matplotlib numpy scipy librosa unidecode inflect\n",
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IZ_tfYCmhXNj",
        "outputId": "8d24a562-05fb-4ced-9e7c-92c6cd6532dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "libsndfile1 is already the newest version (1.0.31-2ubuntu0.2).\n",
            "ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 35 not upgraded.\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (3.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (1.18.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (1.7.3)\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.7/dist-packages (0.6.0)\n",
            "Requirement already satisfied: unidecode in /usr/local/lib/python3.7/dist-packages (1.0.22)\n",
            "Requirement already satisfied: inflect in /usr/local/lib/python3.7/dist-packages (0.2.5)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (1.4.5)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/lib/python3/dist-packages (from matplotlib) (2.4.7)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: audioread>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (3.0.1)\n",
            "Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (1.0.2)\n",
            "Requirement already satisfied: joblib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (1.3.2)\n",
            "Requirement already satisfied: decorator>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (5.1.1)\n",
            "Requirement already satisfied: six>=1.3 in /usr/lib/python3/dist-packages (from librosa) (1.16.0)\n",
            "Requirement already satisfied: resampy>=0.2.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (0.4.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib) (4.7.1)\n",
            "Requirement already satisfied: numba>=0.53 in /usr/local/lib/python3.7/dist-packages (from resampy>=0.2.0->librosa) (0.56.4)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from resampy>=0.2.0->librosa) (5.12.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn!=0.19.0,>=0.14.0->librosa) (3.1.0)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.7/dist-packages (from numba>=0.53->resampy>=0.2.0->librosa) (0.39.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba>=0.53->resampy>=0.2.0->librosa) (68.0.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/lib/python3/dist-packages (from numba>=0.53->resampy>=0.2.0->librosa) (4.6.4)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from importlib-resources->resampy>=0.2.0->librosa) (3.15.0)\n",
            "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement torch (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for torch\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install torch==1.9.0  # Replace with a compatible version\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Di2pLXL3hXCv",
        "outputId": "2101e386-33c7-4714-9be0-30100d8842c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch==1.9.0\n",
            "  Downloading torch-1.9.0-cp37-cp37m-manylinux1_x86_64.whl.metadata (25 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.9.0) (4.7.1)\n",
            "Downloading torch-1.9.0-cp37-cp37m-manylinux1_x86_64.whl (831.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m831.4/831.4 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch\n",
            "Successfully installed torch-1.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch==1.7.1+cu101 torchvision==0.8.2+cu101 -f https://download.pytorch.org/whl/torch_stable.html"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vZot420_CXRT",
        "outputId": "85be98d4-a68c-43d6-f084-e2a9427dcf51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Collecting torch==1.7.1+cu101\n",
            "  Downloading https://download.pytorch.org/whl/cu101/torch-1.7.1%2Bcu101-cp37-cp37m-linux_x86_64.whl (735.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m735.4/735.4 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchvision==0.8.2+cu101\n",
            "  Downloading https://download.pytorch.org/whl/cu101/torchvision-0.8.2%2Bcu101-cp37-cp37m-linux_x86_64.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m80.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.7.1+cu101) (4.7.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.7.1+cu101) (1.18.5)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision==0.8.2+cu101) (9.5.0)\n",
            "Installing collected packages: torch, torchvision\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.9.0\n",
            "    Uninstalling torch-1.9.0:\n",
            "      Successfully uninstalled torch-1.9.0\n",
            "Successfully installed torch-1.7.1+cu101 torchvision-0.8.2+cu101\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Path to the zip file\n",
        "zip_file_path = '/content/drive/MyDrive/processed_data.zip'\n",
        "\n",
        "# Directory where you want to unzip the file\n",
        "unzip_dir = '/content/processed_data'\n",
        "\n",
        "# Ensure the directory exists\n",
        "os.makedirs(unzip_dir, exist_ok=True)\n",
        "\n",
        "# Unzipping the file\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(unzip_dir)\n",
        "\n",
        "print(f\"Files extracted to {unzip_dir}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Gz1vn5ykjlC",
        "outputId": "a9238432-cd61-4a02-e9fd-3651f9f22e80"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files extracted to /content/processed_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "\n",
        "# Specify the directory (current directory)\n",
        "directory = \"/content/processed_data\"\n",
        "\n",
        "# Get all files except .npy files\n",
        "files_to_delete = [f for f in glob.glob(directory + \"/*\") if not f.endswith(\".npy\")]\n",
        "\n",
        "# Delete the files\n",
        "for file in files_to_delete:\n",
        "    try:\n",
        "        os.remove(file)\n",
        "        print(f\"Deleted: {file}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error deleting {file}: {e}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XuNN186yeZmt",
        "outputId": "22d5ab66-481a-4697-cabf-23d293e63068"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deleted: /content/processed_data/metadata.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import random\n",
        "import shutil\n",
        "\n",
        "# Path to your JSON file\n",
        "json_file = '/content/metadata.json'\n",
        "output_dir = '/content/metadata'  # Replace with your desired output directory\n",
        "\n",
        "# Paths for train, valid, and transcripts\n",
        "train_dir = os.path.join(output_dir, 'train')\n",
        "valid_dir = os.path.join(output_dir, 'valid')\n",
        "transcripts_dir = os.path.join(output_dir, 'transcripts')\n",
        "\n",
        "# Create directories if they don't exist\n",
        "os.makedirs(train_dir, exist_ok=True)\n",
        "os.makedirs(valid_dir, exist_ok=True)\n",
        "os.makedirs(transcripts_dir, exist_ok=True)\n",
        "\n",
        "# Load JSON data\n",
        "with open(json_file, 'r') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "# List to hold all filenames and transcriptions\n",
        "all_data = []\n",
        "\n",
        "# Prepare transcripts and create file list\n",
        "for audio_id, details in data.items():\n",
        "    mel_filename = f'{audio_id}.npy'\n",
        "    transcription = details['text']\n",
        "\n",
        "    # Save transcript to a file\n",
        "    with open(os.path.join(transcripts_dir, f'{audio_id}.txt'), 'w') as f:\n",
        "        f.write(transcription)\n",
        "\n",
        "    # Add to the data list\n",
        "    all_data.append(f'{mel_filename}|{transcription}')\n",
        "\n",
        "# Shuffle the data\n",
        "random.shuffle(all_data)\n",
        "\n",
        "# Split data into 90% train and 10% valid\n",
        "train_size = int(0.9 * len(all_data))\n",
        "train_data = all_data[:train_size]\n",
        "valid_data = all_data[train_size:]\n",
        "\n",
        "# Save train and valid files\n",
        "with open(os.path.join(train_dir, 'train.txt'), 'w') as f:\n",
        "    f.write('\\n'.join(train_data))\n",
        "\n",
        "with open(os.path.join(valid_dir, 'valid.txt'), 'w') as f:\n",
        "    f.write('\\n'.join(valid_data))\n",
        "\n",
        "print(\"✅ Train and validation files created successfully.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jtLa8eGGkE6y",
        "outputId": "be97055b-b991-4a50-f900-43af6a9ad161"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Train and validation files created successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "import os\n",
        "\n",
        "def move_subfolder(source_folder, destination_folder):\n",
        "    if not os.path.exists(source_folder):\n",
        "        print(f\"Source folder '{source_folder}' does not exist.\")\n",
        "        return\n",
        "\n",
        "    if not os.path.exists(destination_folder):\n",
        "        os.makedirs(destination_folder)  # Create destination folder if it doesn't exist\n",
        "\n",
        "    folder_name = os.path.basename(source_folder)\n",
        "    destination_path = os.path.join(destination_folder, folder_name)\n",
        "\n",
        "    try:\n",
        "        shutil.move(source_folder, destination_path)\n",
        "        print(f\"Moved '{source_folder}' to '{destination_path}' successfully.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error moving folder: {e}\")\n",
        "\n",
        "# Example usage\n",
        "source = \"/content/mels\"\n",
        "destination = \"/content/tacotron2\"\n",
        "\n",
        "move_subfolder(source, destination)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YZA1cGAmB4sI",
        "outputId": "e8c09943-063f-4d5c-b437-09d72780fbdd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moved '/content/mels' to '/content/tacotron2/mels' successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import librosa\n",
        "import numpy as np\n",
        "\n",
        "# Path to any original audio file from your dataset\n",
        "audio_path = \"/content/LJ001-0005.wav\"\n",
        "\n",
        "# Load the audio\n",
        "y, sr = librosa.load(audio_path, sr=None)\n",
        "\n",
        "print(f\"Detected Sampling Rate: {sr}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mjVzvExsDp1d",
        "outputId": "485aaadd-e362-4246-eb59-32962b5bfde2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detected Sampling Rate: 22050\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Character Embedding Layer\n",
        "class CharacterEmbedding(nn.Module):\n",
        "    def __init__(self, num_chars, embedding_dim):\n",
        "        super(CharacterEmbedding, self).__init__()\n",
        "        self.embedding = nn.Embedding(num_chars, embedding_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.embedding(x)\n",
        "\n",
        "# Encoder: Convolutions + Bidirectional LSTM\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, embedding_dim, conv_channels, lstm_hidden):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.convolutions = nn.Sequential(\n",
        "            nn.Conv1d(embedding_dim, conv_channels, kernel_size=5, padding=2),\n",
        "            nn.BatchNorm1d(conv_channels),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(conv_channels, conv_channels, kernel_size=5, padding=2),\n",
        "            nn.BatchNorm1d(conv_channels),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(conv_channels, conv_channels, kernel_size=5, padding=2),\n",
        "            nn.BatchNorm1d(conv_channels),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.lstm = nn.LSTM(conv_channels, lstm_hidden, batch_first=True, bidirectional=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.transpose(1, 2)  # Change shape for Conv1d (B, T, C) → (B, C, T)\n",
        "        x = self.convolutions(x)\n",
        "        x = x.transpose(1, 2)  # Restore (B, T, C)\n",
        "        x, _ = self.lstm(x)\n",
        "        return x\n",
        "\n",
        "# Prenet: Regularize Decoder Input\n",
        "class Prenet(nn.Module):\n",
        "    def __init__(self, in_dim, sizes=[256, 128]):\n",
        "        super(Prenet, self).__init__()\n",
        "        self.layers = nn.ModuleList([nn.Linear(in_dim, size) for size in sizes])\n",
        "\n",
        "    def forward(self, x):\n",
        "        for layer in self.layers:\n",
        "            x = F.relu(layer(x))\n",
        "            x = F.dropout(x, p=0.5, training=self.training)\n",
        "        return x\n",
        "\n",
        "# Multi-Head Attention (Improved Attention Layer)\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, hidden_dim, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        assert hidden_dim % num_heads == 0\n",
        "        self.num_heads = num_heads\n",
        "        self.dim_per_head = hidden_dim // num_heads\n",
        "\n",
        "        self.query = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.key = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.value = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.out = nn.Linear(hidden_dim, hidden_dim)\n",
        "\n",
        "    def forward(self, query, key, value):\n",
        "        B, T, C = query.size()\n",
        "\n",
        "        # Linear projections for query, key, value\n",
        "        query = self.query(query).view(B, T, self.num_heads, self.dim_per_head).transpose(1, 2)\n",
        "        key = self.key(key).view(B, T, self.num_heads, self.dim_per_head).transpose(1, 2)\n",
        "        value = self.value(value).view(B, T, self.num_heads, self.dim_per_head).transpose(1, 2)\n",
        "\n",
        "        # Scaled Dot-Product Attention\n",
        "        scores = torch.matmul(query, key.transpose(-2, -1)) / (self.dim_per_head ** 0.5)\n",
        "        attn = F.softmax(scores, dim=-1)\n",
        "\n",
        "        # Context Vector\n",
        "        context = torch.matmul(attn, value).transpose(1, 2).contiguous().view(B, T, C)\n",
        "\n",
        "        return self.out(context)\n",
        "\n",
        "# Decoder: LSTM + Attention + Stop Token\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, mel_dim, hidden_dim, num_heads):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        # Assuming a basic decoder structure with a linear layer\n",
        "        self.attention = MultiHeadAttention(hidden_dim, num_heads)  # Replace with your attention mechanism\n",
        "        self.rnn = nn.LSTM(hidden_dim, hidden_dim, batch_first=True)  # Assuming LSTM for the decoder\n",
        "        self.linear = nn.Linear(hidden_dim, mel_dim)  # Convert to mel spectrogram output size\n",
        "\n",
        "    def forward(self, encoder_outputs, mel_input):\n",
        "        # Apply attention and LSTM layers\n",
        "        attention_output = self.attention(encoder_outputs, encoder_outputs, encoder_outputs)  # Simplified attention mechanism\n",
        "        rnn_output, _ = self.rnn(attention_output)\n",
        "\n",
        "        # Pass through the linear layer to get mel spectrogram\n",
        "        mel_output = self.linear(rnn_output)\n",
        "        stop_token = mel_output[:, -1]  # Assuming stop token is the last frame\n",
        "\n",
        "        return mel_output, stop_token\n",
        "\n",
        "\n",
        "# Postnet: Mel-Spectrogram Refinement\n",
        "class Postnet(nn.Module):\n",
        "    def __init__(self, mel_dim, postnet_dim):\n",
        "        super(Postnet, self).__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Conv1d(mel_dim, postnet_dim, kernel_size=5, padding=2),\n",
        "            nn.BatchNorm1d(postnet_dim),\n",
        "            nn.Tanh(),\n",
        "            nn.Conv1d(postnet_dim, mel_dim, kernel_size=5, padding=2)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x.transpose(1, 2)).transpose(1, 2)\n",
        "\n",
        "# Full Tacotron 2 Model\n",
        "class Tacotron2(nn.Module):\n",
        "    def __init__(self, num_chars, embedding_dim, mel_dim, hidden_dim, num_heads, postnet_dim):\n",
        "        super(Tacotron2, self).__init__()\n",
        "        self.embedding = CharacterEmbedding(num_chars, embedding_dim)\n",
        "        self.encoder = Encoder(embedding_dim, hidden_dim, hidden_dim)\n",
        "        self.decoder = Decoder(mel_dim, hidden_dim, num_heads)\n",
        "        self.postnet = Postnet(mel_dim, postnet_dim)\n",
        "\n",
        "    def forward(self, text, mel_input):\n",
        "        # Embedding + Encoding\n",
        "        embedded_text = self.embedding(text)\n",
        "        encoder_outputs = self.encoder(embedded_text)\n",
        "\n",
        "        # Decode to mel-spectrogram + stop token\n",
        "        mel_output, stop_token = self.decoder(encoder_outputs, mel_input)\n",
        "\n",
        "        # Postnet refinement with residual learning\n",
        "        mel_output = mel_output + self.postnet(mel_output)\n",
        "\n",
        "        return mel_output, stop_token\n",
        "\n",
        "\n",
        "# Updated Training Loop with Padding / Cropping for Mismatch Handling\n",
        "def train_model(model, train_loader, optimizer, criterion_mel, criterion_stop, num_epochs=100):\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        for i, (text, mel, mel_shape) in enumerate(train_loader):\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            mel_output, stop_token = model(text, mel)\n",
        "\n",
        "            # Handle target mel spectrogram size mismatch by padding/cropping\n",
        "            mel_length = mel_output.size(1)\n",
        "            target_length = mel.size(1)\n",
        "\n",
        "            # Padding or cropping the target mel spectrogram to match predicted mel sequence length\n",
        "            if target_length < mel_length:\n",
        "                mel = mel[:, :mel_length, :]  # Crop the target mel\n",
        "            elif target_length > mel_length:\n",
        "                padding_size = target_length - mel_length\n",
        "                mel = F.pad(mel, (0, padding_size))  # Pad the target mel\n",
        "\n",
        "            # Compute losses\n",
        "            mel_loss = criterion_mel(mel_output, mel)\n",
        "            stop_token_loss = criterion_stop(stop_token.squeeze(), torch.ones_like(stop_token.squeeze()))  # assuming all tokens are '1'\n",
        "\n",
        "            loss = mel_loss + stop_token_loss\n",
        "            loss.backward()\n",
        "\n",
        "            # Optimization step\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader)}\")\n",
        "\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "2THxD7EnFfdr"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch import nn\n",
        "from tqdm import tqdm\n",
        "import torch.nn.utils.rnn as rnn_utils\n",
        "\n",
        "# Path to dataset\n",
        "DATA_PATH = '/content/processed_data'  # Update this path to where your data is stored\n",
        "METADATA_PATH = '/content/metadata.json'  # Path to metadata.json\n",
        "\n",
        "# Load metadata from JSON\n",
        "with open(METADATA_PATH, 'r') as f:\n",
        "    metadata = json.load(f)\n",
        "\n",
        "# Custom Dataset class\n",
        "class Tacotron2Dataset(Dataset):\n",
        "    def __init__(self, data_path, metadata, max_length=1500):\n",
        "        self.data_path = data_path\n",
        "        self.metadata = metadata\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.metadata)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        file_id = list(self.metadata.keys())[idx]\n",
        "        mel_path = os.path.join(self.data_path, f'{file_id}.npy')\n",
        "        mel_spectrogram = np.load(mel_path)\n",
        "\n",
        "        text = self.metadata[file_id]['text']\n",
        "        mel_shape = self.metadata[file_id]['mel_shape']\n",
        "\n",
        "        # Convert text to indices (here we assume text is preprocessed to indices)\n",
        "        text_indices = self.text_to_sequence(text)\n",
        "\n",
        "        if len(text_indices) > self.max_length:\n",
        "            text_indices = text_indices[:self.max_length]\n",
        "\n",
        "        return torch.tensor(text_indices), torch.tensor(mel_spectrogram), torch.tensor(mel_shape)\n",
        "\n",
        "    def text_to_sequence(self, text):\n",
        "        # Convert text to sequence of indices based on a basic char-to-index mapping\n",
        "        # Extend this if necessary for more characters or a different language\n",
        "        char_to_idx = {chr(i + 97): i for i in range(26)}  # Basic char-to-index for a-z\n",
        "        char_to_idx.update({' ': 26, ',': 27, '.': 28})  # Add some punctuation as examples\n",
        "        text_indices = [char_to_idx.get(c, 0) for c in text.lower()]  # Default to 0 for unknown chars\n",
        "        return text_indices\n",
        "\n",
        "# Collate function to pad the sequences to the max length in each batch\n",
        "def collate_fn(batch):\n",
        "    # Sort by sequence length in descending order\n",
        "    batch = sorted(batch, key=lambda x: len(x[0]), reverse=True)\n",
        "\n",
        "    # Separate the inputs, mel spectrograms, and mel shapes\n",
        "    text_sequences, mel_spectrograms, mel_shapes = zip(*batch)\n",
        "\n",
        "    # Pad the text sequences\n",
        "    text_sequences_padded = rnn_utils.pad_sequence(text_sequences, batch_first=True, padding_value=0)\n",
        "\n",
        "    # Pad mel spectrograms to the maximum length in the batch\n",
        "    mel_lengths = [mel.shape[1] for mel in mel_spectrograms]  # Get the time dimension size\n",
        "    max_mel_length = max(mel_lengths)  # Find the max length for padding\n",
        "\n",
        "    padded_mel_spectrograms = []\n",
        "    for mel in mel_spectrograms:\n",
        "        padding_size = max_mel_length - mel.shape[1]\n",
        "        padded_mel = np.pad(mel, ((0, 0), (0, padding_size)), mode='constant')\n",
        "        padded_mel_spectrograms.append(torch.tensor(padded_mel))\n",
        "\n",
        "    # Stack mel spectrograms and mel shapes into batches\n",
        "    mel_spectrograms = torch.stack(padded_mel_spectrograms, dim=0)\n",
        "    mel_shapes = torch.stack(mel_shapes, dim=0)\n",
        "\n",
        "    return text_sequences_padded, mel_spectrograms, mel_shapes\n",
        "\n",
        "\n",
        "# Initialize Dataset and DataLoader with the custom collate function\n",
        "train_dataset = Tacotron2Dataset(DATA_PATH, metadata)\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=2, collate_fn=collate_fn)\n",
        "\n",
        "# Model definition as in your code\n",
        "class Tacotron2(nn.Module):\n",
        "    def __init__(self, num_chars=30, embedding_dim=256, mel_dim=80, hidden_dim=1024, num_heads=8, postnet_dim=512):\n",
        "        super(Tacotron2, self).__init__()\n",
        "        self.embedding = nn.Embedding(num_chars, embedding_dim)\n",
        "        self.encoder = Encoder(embedding_dim, hidden_dim, hidden_dim)\n",
        "\n",
        "        # Add a linear layer to match encoder output (2048) to decoder input size\n",
        "        self.encoder_projection = nn.Linear(2048, hidden_dim)  # Reduce from 2048 to hidden_dim (1024)\n",
        "\n",
        "        self.decoder = Decoder(mel_dim, hidden_dim, num_heads)\n",
        "        self.postnet = Postnet(mel_dim, postnet_dim)\n",
        "\n",
        "    def forward(self, text, mel_input):\n",
        "        # Embed the text\n",
        "        embedded_text = self.embedding(text)\n",
        "        print(f\"Embedded Text Shape: {embedded_text.shape}\")  # Print shape\n",
        "\n",
        "        encoder_outputs = self.encoder(embedded_text)\n",
        "        print(f\"Encoder Output Shape: {encoder_outputs.shape}\")  # Print shape\n",
        "\n",
        "        # Project encoder output to decoder input size (hidden_dim)\n",
        "        encoder_outputs = self.encoder_projection(encoder_outputs)\n",
        "        print(f\"Projected Encoder Output Shape: {encoder_outputs.shape}\")  # Print shape\n",
        "\n",
        "        # Pass encoder outputs directly to decoder without flattening\n",
        "        mel_output, stop_token = self.decoder(encoder_outputs, mel_input)\n",
        "        print(f\"Mel Output Shape: {mel_output.shape}, Stop Token Shape: {stop_token.shape}\")  # Print shape\n",
        "\n",
        "        mel_output = mel_output + self.postnet(mel_output)\n",
        "        return mel_output, stop_token\n",
        "\n",
        "\n",
        "# Initialize model, optimizer, and loss functions\n",
        "model = Tacotron2()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "# Loss function: MSE for mel spectrograms, binary cross-entropy for stop token\n",
        "criterion_mel = nn.MSELoss(reduction='none')  # No reduction to handle padding manually\n",
        "criterion_stop = nn.BCEWithLogitsLoss()\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 100\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for i, (text, mel, mel_shape) in enumerate(tqdm(train_loader)):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        mel_output, stop_token = model(text, mel)\n",
        "\n",
        "        # Crop or pad the mel spectrogram to match predicted length\n",
        "        mel_lengths = [mel.size(1) for mel in mel]\n",
        "        mel_output = mel_output[:, :max(mel_lengths), :]  # Crop if output length is greater\n",
        "\n",
        "        # Compute losses\n",
        "        mel_loss = criterion_mel(mel_output, mel)  # Loss will be computed element-wise\n",
        "        stop_token_loss = criterion_stop(stop_token.squeeze(), torch.ones_like(stop_token.squeeze()))  # assuming all tokens are '1'\n",
        "\n",
        "        # Mask out the loss for padded sections\n",
        "        mel_loss = mel_loss.sum() / mel_loss.size(0)  # Averaging across batch size\n",
        "\n",
        "        loss = mel_loss + stop_token_loss\n",
        "        loss.backward()\n",
        "\n",
        "        # Optimization step\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader)}\")\n",
        "\n",
        "    # Save model checkpoint every few epochs\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        torch.save(model.state_dict(), f\"/content/tacotron2_epoch_{epoch+1}.pth\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "Y_Nn9vCNbyGW",
        "outputId": "2d11da28-bb28-46c6-cbac-a5bc8994bdb2"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "expected an indented block after function definition on line 29 (<ipython-input-28-f2166496de5b>, line 30)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-28-f2166496de5b>\"\u001b[0;36m, line \u001b[0;32m30\u001b[0m\n\u001b[0;31m    file_id = list(self.metadata.keys())[idx]\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block after function definition on line 29\n"
          ]
        }
      ]
    }
  ]
}